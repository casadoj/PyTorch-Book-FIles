{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "EZay5pyZMWBg"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torchvision import datasets, transforms\n",
        "from torch.utils.data import DataLoader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-images-idx3-ubyte.gz\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-images-idx3-ubyte.gz to ./data/FashionMNIST/raw/train-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100.0%\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Extracting ./data/FashionMNIST/raw/train-images-idx3-ubyte.gz to ./data/FashionMNIST/raw\n",
            "\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-labels-idx1-ubyte.gz\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-labels-idx1-ubyte.gz to ./data/FashionMNIST/raw/train-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100.0%\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Extracting ./data/FashionMNIST/raw/train-labels-idx1-ubyte.gz to ./data/FashionMNIST/raw\n",
            "\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-images-idx3-ubyte.gz\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-images-idx3-ubyte.gz to ./data/FashionMNIST/raw/t10k-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100.0%\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Extracting ./data/FashionMNIST/raw/t10k-images-idx3-ubyte.gz to ./data/FashionMNIST/raw\n",
            "\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100.0%"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-labels-idx1-ubyte.gz to ./data/FashionMNIST/raw/t10k-labels-idx1-ubyte.gz\n",
            "Extracting ./data/FashionMNIST/raw/t10k-labels-idx1-ubyte.gz to ./data/FashionMNIST/raw\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "# Load the dataset\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor()  # Automatically converts to tensor and scales to [0, 1]\n",
        "])\n",
        "train_dataset = datasets.FashionMNIST(root='./data', train=True, download=True, transform=transform)\n",
        "test_dataset = datasets.FashionMNIST(root='./data', train=False, download=True, transform=transform)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Define the model\n",
        "class FashionMNISTModel(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(FashionMNISTModel, self).__init__()\n",
        "        self.flatten = nn.Flatten()\n",
        "        self.linear_relu_stack = nn.Sequential(\n",
        "            nn.Linear(28*28, 128),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(128, 10),\n",
        "            nn.LogSoftmax(dim=1)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.flatten(x)\n",
        "        logits = self.linear_relu_stack(x)\n",
        "        return logits\n",
        "\n",
        "model = FashionMNISTModel()\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    model = model.to('cuda')\n",
        "    \n",
        "\n",
        "# Define the loss function and optimizer\n",
        "loss_function = nn.NLLLoss()\n",
        "optimizer = optim.Adam(model.parameters())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1\n",
            "-------------------------------\n",
            "loss: 2.291573  [    0/60000]\n",
            "loss: 0.652354  [ 6400/60000]\n",
            "loss: 0.478874  [12800/60000]\n",
            "loss: 0.349336  [19200/60000]\n",
            "loss: 0.510502  [25600/60000]\n",
            "loss: 0.729749  [32000/60000]\n",
            "loss: 0.532373  [38400/60000]\n",
            "loss: 0.374619  [44800/60000]\n",
            "loss: 0.330385  [51200/60000]\n",
            "loss: 0.434690  [57600/60000]\n",
            "Epoch 2\n",
            "-------------------------------\n",
            "loss: 0.257837  [    0/60000]\n",
            "loss: 0.396995  [ 6400/60000]\n",
            "loss: 0.262595  [12800/60000]\n",
            "loss: 0.258004  [19200/60000]\n",
            "loss: 0.367092  [25600/60000]\n",
            "loss: 0.317387  [32000/60000]\n",
            "loss: 0.512914  [38400/60000]\n",
            "loss: 0.336108  [44800/60000]\n",
            "loss: 0.302062  [51200/60000]\n",
            "loss: 0.334314  [57600/60000]\n",
            "Epoch 3\n",
            "-------------------------------\n",
            "loss: 0.361124  [    0/60000]\n",
            "loss: 0.395251  [ 6400/60000]\n",
            "loss: 0.356017  [12800/60000]\n",
            "loss: 0.335511  [19200/60000]\n",
            "loss: 0.495257  [25600/60000]\n",
            "loss: 0.459716  [32000/60000]\n",
            "loss: 0.650583  [38400/60000]\n",
            "loss: 0.351644  [44800/60000]\n",
            "loss: 0.257056  [51200/60000]\n",
            "loss: 0.246612  [57600/60000]\n",
            "Epoch 4\n",
            "-------------------------------\n",
            "loss: 0.407690  [    0/60000]\n",
            "loss: 0.369038  [ 6400/60000]\n",
            "loss: 0.440180  [12800/60000]\n",
            "loss: 0.322768  [19200/60000]\n",
            "loss: 0.213350  [25600/60000]\n",
            "loss: 0.353930  [32000/60000]\n",
            "loss: 0.371973  [38400/60000]\n",
            "loss: 0.313123  [44800/60000]\n",
            "loss: 0.301404  [51200/60000]\n",
            "loss: 0.520920  [57600/60000]\n",
            "Epoch 5\n",
            "-------------------------------\n",
            "loss: 0.376760  [    0/60000]\n",
            "loss: 0.350316  [ 6400/60000]\n",
            "loss: 0.296084  [12800/60000]\n",
            "loss: 0.378201  [19200/60000]\n",
            "loss: 0.297524  [25600/60000]\n",
            "loss: 0.494304  [32000/60000]\n",
            "loss: 0.311006  [38400/60000]\n",
            "loss: 0.413256  [44800/60000]\n",
            "loss: 0.190092  [51200/60000]\n",
            "loss: 0.337222  [57600/60000]\n",
            "Done!\n"
          ]
        }
      ],
      "source": [
        "# Train the model\n",
        "def train(dataloader, model, loss_fn, optimizer):\n",
        "    size = len(dataloader.dataset)\n",
        "    model.train()\n",
        "    for batch, (X, y) in enumerate(dataloader):\n",
        "        if torch.cuda.is_available():\n",
        "            X = X.to('cuda')\n",
        "            y = y.to('cuda')\n",
        "        # Compute prediction and loss\n",
        "        pred = model(X)\n",
        "        loss = loss_fn(pred, y)\n",
        "\n",
        "        # Backpropagation\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        if batch % 100 == 0:\n",
        "            loss, current = loss.item(), batch * len(X)\n",
        "            print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n",
        "\n",
        "# Training process\n",
        "epochs = 5\n",
        "for t in range(epochs):\n",
        "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
        "    train(train_loader, model, loss_function, optimizer)\n",
        "print(\"Done!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "ozzKgn7_O_66"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Test Error: \n",
            " Accuracy: 86.8%, Avg loss: 0.361934 \n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Function to test the model\n",
        "def test(dataloader, model):\n",
        "    size = len(dataloader.dataset)\n",
        "    num_batches = len(dataloader)\n",
        "    model.eval()  # Set the model to evaluation mode\n",
        "    test_loss, correct = 0, 0\n",
        "    with torch.no_grad():\n",
        "        for X, y in dataloader:\n",
        "            if torch.cuda.is_available():\n",
        "                X, y = X.to('cuda'), y.to('cuda')\n",
        "            pred = model(X)\n",
        "            test_loss += loss_function(pred, y).item()\n",
        "            correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
        "    test_loss /= num_batches\n",
        "    correct /= size\n",
        "    print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")\n",
        "\n",
        "# Evaluate the model\n",
        "test(test_loader, model)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "HNZ-FF6NPSTe"
      },
      "outputs": [],
      "source": [
        "# CHECK SINGLE PREDICTION AND PLOT\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def predict_single_image(image, label, model):\n",
        "    # Set the model to evaluation mode\n",
        "    model.eval()\n",
        "\n",
        "    # Image needs to be unsqueezed as the model expects a batch dimension\n",
        "    image = image.unsqueeze(0)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        if torch.cuda.is_available():\n",
        "            image = image.to('cuda')\n",
        "        prediction = model(image)\n",
        "        print(prediction)\n",
        "        predicted_label = prediction.argmax(1).item()\n",
        "\n",
        "    # Display the image and predictions\n",
        "    plt.imshow(image.to('cpu').squeeze(), cmap='gray')\n",
        "    plt.title(f'Predicted: {predicted_label}, Actual: {label}')\n",
        "    plt.show()\n",
        "\n",
        "    return predicted_label"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([[ -1.1075, -10.0689,  -4.1681,  -7.0006, -11.1615, -16.0137,  -0.4279,\n",
            "         -19.7766,  -6.6250, -18.6619]], device='cuda:0')\n"
          ]
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The model predicted 6, and the actual label is 0.\n"
          ]
        }
      ],
      "source": [
        "# Choose an image from the test set\n",
        "image, label = test_dataset[1000]  # Change index to test different images\n",
        "\n",
        "# Predict the class for the chosen image\n",
        "predicted_label = predict_single_image(image, label, model)\n",
        "print(f\"The model predicted {predicted_label}, and the actual label is {label}.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "Ve9Y_VI-yaCq"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1\n",
            "-------------------------------\n",
            "Batch 0, Loss: 2.314224, Accuracy: 23.44% [    0/60000]\n",
            "Batch 100, Loss: 0.972206, Accuracy: 68.30% [ 6400/60000]\n",
            "Batch 200, Loss: 0.780996, Accuracy: 73.87% [12800/60000]\n",
            "Batch 300, Loss: 0.698478, Accuracy: 76.42% [19200/60000]\n",
            "Batch 400, Loss: 0.648113, Accuracy: 78.03% [25600/60000]\n",
            "Batch 500, Loss: 0.617895, Accuracy: 78.89% [32000/60000]\n",
            "Batch 600, Loss: 0.596323, Accuracy: 79.52% [38400/60000]\n",
            "Batch 700, Loss: 0.576943, Accuracy: 80.20% [44800/60000]\n",
            "Batch 800, Loss: 0.561502, Accuracy: 80.70% [51200/60000]\n",
            "Batch 900, Loss: 0.550031, Accuracy: 81.10% [57600/60000]\n",
            "Epoch 2\n",
            "-------------------------------\n",
            "Batch 0, Loss: 0.450240, Accuracy: 85.94% [    0/60000]\n",
            "Batch 100, Loss: 0.423368, Accuracy: 85.21% [ 6400/60000]\n",
            "Batch 200, Loss: 0.421220, Accuracy: 85.15% [12800/60000]\n",
            "Batch 300, Loss: 0.416529, Accuracy: 85.39% [19200/60000]\n",
            "Batch 400, Loss: 0.417957, Accuracy: 85.29% [25600/60000]\n",
            "Batch 500, Loss: 0.417394, Accuracy: 85.20% [32000/60000]\n",
            "Batch 600, Loss: 0.414364, Accuracy: 85.27% [38400/60000]\n",
            "Batch 700, Loss: 0.411899, Accuracy: 85.35% [44800/60000]\n",
            "Batch 800, Loss: 0.407134, Accuracy: 85.52% [51200/60000]\n",
            "Batch 900, Loss: 0.404732, Accuracy: 85.65% [57600/60000]\n",
            "Epoch 3\n",
            "-------------------------------\n",
            "Batch 0, Loss: 0.300664, Accuracy: 87.50% [    0/60000]\n",
            "Batch 100, Loss: 0.367319, Accuracy: 86.65% [ 6400/60000]\n",
            "Batch 200, Loss: 0.365675, Accuracy: 86.71% [12800/60000]\n",
            "Batch 300, Loss: 0.363546, Accuracy: 86.77% [19200/60000]\n",
            "Batch 400, Loss: 0.365231, Accuracy: 86.84% [25600/60000]\n",
            "Batch 500, Loss: 0.366284, Accuracy: 86.86% [32000/60000]\n",
            "Batch 600, Loss: 0.364796, Accuracy: 86.93% [38400/60000]\n",
            "Batch 700, Loss: 0.363660, Accuracy: 86.93% [44800/60000]\n",
            "Batch 800, Loss: 0.360242, Accuracy: 87.09% [51200/60000]\n",
            "Batch 900, Loss: 0.358816, Accuracy: 87.12% [57600/60000]\n",
            "Epoch 4\n",
            "-------------------------------\n",
            "Batch 0, Loss: 0.294781, Accuracy: 89.06% [    0/60000]\n",
            "Batch 100, Loss: 0.341353, Accuracy: 87.31% [ 6400/60000]\n",
            "Batch 200, Loss: 0.337158, Accuracy: 87.63% [12800/60000]\n",
            "Batch 300, Loss: 0.338721, Accuracy: 87.52% [19200/60000]\n",
            "Batch 400, Loss: 0.335571, Accuracy: 87.62% [25600/60000]\n",
            "Batch 500, Loss: 0.334776, Accuracy: 87.70% [32000/60000]\n",
            "Batch 600, Loss: 0.334175, Accuracy: 87.77% [38400/60000]\n",
            "Batch 700, Loss: 0.333889, Accuracy: 87.81% [44800/60000]\n",
            "Batch 800, Loss: 0.332514, Accuracy: 87.79% [51200/60000]\n",
            "Batch 900, Loss: 0.333219, Accuracy: 87.77% [57600/60000]\n",
            "Epoch 5\n",
            "-------------------------------\n",
            "Batch 0, Loss: 0.304630, Accuracy: 89.06% [    0/60000]\n",
            "Batch 100, Loss: 0.312814, Accuracy: 88.80% [ 6400/60000]\n",
            "Batch 200, Loss: 0.307718, Accuracy: 88.85% [12800/60000]\n",
            "Batch 300, Loss: 0.308783, Accuracy: 88.73% [19200/60000]\n",
            "Batch 400, Loss: 0.306851, Accuracy: 88.76% [25600/60000]\n",
            "Batch 500, Loss: 0.305039, Accuracy: 88.81% [32000/60000]\n",
            "Batch 600, Loss: 0.305395, Accuracy: 88.78% [38400/60000]\n",
            "Batch 700, Loss: 0.309077, Accuracy: 88.65% [44800/60000]\n",
            "Batch 800, Loss: 0.309720, Accuracy: 88.68% [51200/60000]\n",
            "Batch 900, Loss: 0.309221, Accuracy: 88.70% [57600/60000]\n",
            "Epoch 6\n",
            "-------------------------------\n",
            "Batch 0, Loss: 0.329847, Accuracy: 89.06% [    0/60000]\n",
            "Batch 100, Loss: 0.301256, Accuracy: 89.14% [ 6400/60000]\n",
            "Batch 200, Loss: 0.294427, Accuracy: 89.31% [12800/60000]\n",
            "Batch 300, Loss: 0.295419, Accuracy: 89.30% [19200/60000]\n",
            "Batch 400, Loss: 0.295127, Accuracy: 89.33% [25600/60000]\n",
            "Batch 500, Loss: 0.293372, Accuracy: 89.36% [32000/60000]\n",
            "Batch 600, Loss: 0.293389, Accuracy: 89.39% [38400/60000]\n",
            "Batch 700, Loss: 0.293705, Accuracy: 89.38% [44800/60000]\n",
            "Batch 800, Loss: 0.294595, Accuracy: 89.29% [51200/60000]\n",
            "Batch 900, Loss: 0.293717, Accuracy: 89.33% [57600/60000]\n",
            "Epoch 7\n",
            "-------------------------------\n",
            "Batch 0, Loss: 0.423954, Accuracy: 87.50% [    0/60000]\n",
            "Batch 100, Loss: 0.290902, Accuracy: 89.81% [ 6400/60000]\n",
            "Batch 200, Loss: 0.281275, Accuracy: 89.80% [12800/60000]\n",
            "Batch 300, Loss: 0.280617, Accuracy: 89.74% [19200/60000]\n",
            "Batch 400, Loss: 0.277399, Accuracy: 89.95% [25600/60000]\n",
            "Batch 500, Loss: 0.276302, Accuracy: 89.99% [32000/60000]\n",
            "Batch 600, Loss: 0.278089, Accuracy: 89.94% [38400/60000]\n",
            "Batch 700, Loss: 0.279054, Accuracy: 89.92% [44800/60000]\n",
            "Batch 800, Loss: 0.281927, Accuracy: 89.81% [51200/60000]\n",
            "Batch 900, Loss: 0.281864, Accuracy: 89.78% [57600/60000]\n",
            "Epoch 8\n",
            "-------------------------------\n",
            "Batch 0, Loss: 0.259982, Accuracy: 90.62% [    0/60000]\n",
            "Batch 100, Loss: 0.266640, Accuracy: 90.01% [ 6400/60000]\n",
            "Batch 200, Loss: 0.260595, Accuracy: 90.19% [12800/60000]\n",
            "Batch 300, Loss: 0.266925, Accuracy: 90.03% [19200/60000]\n",
            "Batch 400, Loss: 0.266944, Accuracy: 90.17% [25600/60000]\n",
            "Batch 500, Loss: 0.266314, Accuracy: 90.16% [32000/60000]\n",
            "Batch 600, Loss: 0.267983, Accuracy: 90.14% [38400/60000]\n",
            "Batch 700, Loss: 0.269565, Accuracy: 90.08% [44800/60000]\n",
            "Batch 800, Loss: 0.271249, Accuracy: 90.06% [51200/60000]\n",
            "Batch 900, Loss: 0.271523, Accuracy: 90.03% [57600/60000]\n",
            "Epoch 9\n",
            "-------------------------------\n",
            "Batch 0, Loss: 0.421470, Accuracy: 84.38% [    0/60000]\n",
            "Batch 100, Loss: 0.265668, Accuracy: 90.13% [ 6400/60000]\n",
            "Batch 200, Loss: 0.267786, Accuracy: 89.96% [12800/60000]\n",
            "Batch 300, Loss: 0.263600, Accuracy: 90.27% [19200/60000]\n",
            "Batch 400, Loss: 0.262185, Accuracy: 90.28% [25600/60000]\n",
            "Batch 500, Loss: 0.264164, Accuracy: 90.18% [32000/60000]\n",
            "Batch 600, Loss: 0.263300, Accuracy: 90.23% [38400/60000]\n",
            "Batch 700, Loss: 0.260961, Accuracy: 90.39% [44800/60000]\n",
            "Batch 800, Loss: 0.261582, Accuracy: 90.37% [51200/60000]\n",
            "Batch 900, Loss: 0.260227, Accuracy: 90.40% [57600/60000]\n",
            "Epoch 10\n",
            "-------------------------------\n",
            "Batch 0, Loss: 0.264413, Accuracy: 90.62% [    0/60000]\n",
            "Batch 100, Loss: 0.246987, Accuracy: 91.09% [ 6400/60000]\n",
            "Batch 200, Loss: 0.251307, Accuracy: 90.63% [12800/60000]\n",
            "Batch 300, Loss: 0.247956, Accuracy: 90.72% [19200/60000]\n",
            "Batch 400, Loss: 0.250645, Accuracy: 90.61% [25600/60000]\n",
            "Batch 500, Loss: 0.254289, Accuracy: 90.51% [32000/60000]\n",
            "Batch 600, Loss: 0.253891, Accuracy: 90.57% [38400/60000]\n",
            "Batch 700, Loss: 0.252770, Accuracy: 90.63% [44800/60000]\n",
            "Batch 800, Loss: 0.254103, Accuracy: 90.59% [51200/60000]\n",
            "Batch 900, Loss: 0.252531, Accuracy: 90.67% [57600/60000]\n",
            "Epoch 11\n",
            "-------------------------------\n",
            "Batch 0, Loss: 0.309005, Accuracy: 89.06% [    0/60000]\n",
            "Batch 100, Loss: 0.246369, Accuracy: 91.15% [ 6400/60000]\n",
            "Batch 200, Loss: 0.243513, Accuracy: 91.15% [12800/60000]\n",
            "Batch 300, Loss: 0.242008, Accuracy: 91.16% [19200/60000]\n",
            "Batch 400, Loss: 0.243252, Accuracy: 91.04% [25600/60000]\n",
            "Batch 500, Loss: 0.241714, Accuracy: 91.11% [32000/60000]\n",
            "Batch 600, Loss: 0.243336, Accuracy: 91.10% [38400/60000]\n",
            "Batch 700, Loss: 0.245161, Accuracy: 91.00% [44800/60000]\n",
            "Batch 800, Loss: 0.243772, Accuracy: 91.06% [51200/60000]\n",
            "Batch 900, Loss: 0.244748, Accuracy: 91.05% [57600/60000]\n",
            "Epoch 12\n",
            "-------------------------------\n",
            "Batch 0, Loss: 0.253949, Accuracy: 89.06% [    0/60000]\n",
            "Batch 100, Loss: 0.242374, Accuracy: 91.20% [ 6400/60000]\n",
            "Batch 200, Loss: 0.239400, Accuracy: 91.20% [12800/60000]\n",
            "Batch 300, Loss: 0.235940, Accuracy: 91.23% [19200/60000]\n",
            "Batch 400, Loss: 0.234987, Accuracy: 91.27% [25600/60000]\n",
            "Batch 500, Loss: 0.235721, Accuracy: 91.27% [32000/60000]\n",
            "Batch 600, Loss: 0.236867, Accuracy: 91.22% [38400/60000]\n",
            "Batch 700, Loss: 0.236252, Accuracy: 91.26% [44800/60000]\n",
            "Batch 800, Loss: 0.235244, Accuracy: 91.32% [51200/60000]\n",
            "Batch 900, Loss: 0.234952, Accuracy: 91.30% [57600/60000]\n",
            "Epoch 13\n",
            "-------------------------------\n",
            "Batch 0, Loss: 0.260159, Accuracy: 92.19% [    0/60000]\n",
            "Batch 100, Loss: 0.237551, Accuracy: 91.14% [ 6400/60000]\n",
            "Batch 200, Loss: 0.232066, Accuracy: 91.34% [12800/60000]\n",
            "Batch 300, Loss: 0.232356, Accuracy: 91.37% [19200/60000]\n",
            "Batch 400, Loss: 0.230302, Accuracy: 91.51% [25600/60000]\n",
            "Batch 500, Loss: 0.230313, Accuracy: 91.52% [32000/60000]\n",
            "Batch 600, Loss: 0.229470, Accuracy: 91.52% [38400/60000]\n",
            "Batch 700, Loss: 0.227978, Accuracy: 91.52% [44800/60000]\n",
            "Batch 800, Loss: 0.229206, Accuracy: 91.46% [51200/60000]\n",
            "Batch 900, Loss: 0.228246, Accuracy: 91.54% [57600/60000]\n",
            "Epoch 14\n",
            "-------------------------------\n",
            "Batch 0, Loss: 0.202703, Accuracy: 95.31% [    0/60000]\n",
            "Batch 100, Loss: 0.212467, Accuracy: 92.13% [ 6400/60000]\n",
            "Batch 200, Loss: 0.213218, Accuracy: 92.01% [12800/60000]\n",
            "Batch 300, Loss: 0.216153, Accuracy: 91.96% [19200/60000]\n",
            "Batch 400, Loss: 0.218622, Accuracy: 91.88% [25600/60000]\n",
            "Batch 500, Loss: 0.219550, Accuracy: 91.93% [32000/60000]\n",
            "Batch 600, Loss: 0.219913, Accuracy: 91.89% [38400/60000]\n",
            "Batch 700, Loss: 0.222120, Accuracy: 91.78% [44800/60000]\n",
            "Batch 800, Loss: 0.221984, Accuracy: 91.79% [51200/60000]\n",
            "Batch 900, Loss: 0.220953, Accuracy: 91.82% [57600/60000]\n",
            "Epoch 15\n",
            "-------------------------------\n",
            "Batch 0, Loss: 0.458488, Accuracy: 87.50% [    0/60000]\n",
            "Batch 100, Loss: 0.219354, Accuracy: 92.05% [ 6400/60000]\n",
            "Batch 200, Loss: 0.214135, Accuracy: 92.08% [12800/60000]\n",
            "Batch 300, Loss: 0.213871, Accuracy: 92.01% [19200/60000]\n",
            "Batch 400, Loss: 0.216660, Accuracy: 91.96% [25600/60000]\n",
            "Batch 500, Loss: 0.215726, Accuracy: 91.99% [32000/60000]\n",
            "Batch 600, Loss: 0.215883, Accuracy: 92.01% [38400/60000]\n",
            "Batch 700, Loss: 0.216689, Accuracy: 91.98% [44800/60000]\n",
            "Batch 800, Loss: 0.217237, Accuracy: 92.01% [51200/60000]\n",
            "Batch 900, Loss: 0.216191, Accuracy: 92.02% [57600/60000]\n",
            "Epoch 16\n",
            "-------------------------------\n",
            "Batch 0, Loss: 0.358814, Accuracy: 89.06% [    0/60000]\n",
            "Batch 100, Loss: 0.206270, Accuracy: 92.62% [ 6400/60000]\n",
            "Batch 200, Loss: 0.204758, Accuracy: 92.71% [12800/60000]\n",
            "Batch 300, Loss: 0.209673, Accuracy: 92.49% [19200/60000]\n",
            "Batch 400, Loss: 0.207617, Accuracy: 92.50% [25600/60000]\n",
            "Batch 500, Loss: 0.209400, Accuracy: 92.40% [32000/60000]\n",
            "Batch 600, Loss: 0.208259, Accuracy: 92.37% [38400/60000]\n",
            "Batch 700, Loss: 0.210086, Accuracy: 92.29% [44800/60000]\n",
            "Batch 800, Loss: 0.210405, Accuracy: 92.31% [51200/60000]\n",
            "Batch 900, Loss: 0.209505, Accuracy: 92.36% [57600/60000]\n",
            "Epoch 17\n",
            "-------------------------------\n",
            "Batch 0, Loss: 0.181190, Accuracy: 93.75% [    0/60000]\n",
            "Batch 100, Loss: 0.212408, Accuracy: 92.11% [ 6400/60000]\n",
            "Batch 200, Loss: 0.205306, Accuracy: 92.45% [12800/60000]\n",
            "Batch 300, Loss: 0.201140, Accuracy: 92.48% [19200/60000]\n",
            "Batch 400, Loss: 0.200866, Accuracy: 92.54% [25600/60000]\n",
            "Batch 500, Loss: 0.202508, Accuracy: 92.46% [32000/60000]\n",
            "Batch 600, Loss: 0.201082, Accuracy: 92.48% [38400/60000]\n",
            "Batch 700, Loss: 0.203097, Accuracy: 92.47% [44800/60000]\n",
            "Batch 800, Loss: 0.202253, Accuracy: 92.50% [51200/60000]\n",
            "Batch 900, Loss: 0.202404, Accuracy: 92.49% [57600/60000]\n",
            "Epoch 18\n",
            "-------------------------------\n",
            "Batch 0, Loss: 0.193647, Accuracy: 96.88% [    0/60000]\n",
            "Batch 100, Loss: 0.193251, Accuracy: 92.98% [ 6400/60000]\n",
            "Batch 200, Loss: 0.195381, Accuracy: 92.83% [12800/60000]\n",
            "Batch 300, Loss: 0.197914, Accuracy: 92.63% [19200/60000]\n",
            "Batch 400, Loss: 0.196879, Accuracy: 92.66% [25600/60000]\n",
            "Batch 500, Loss: 0.197120, Accuracy: 92.68% [32000/60000]\n",
            "Batch 600, Loss: 0.195065, Accuracy: 92.77% [38400/60000]\n",
            "Batch 700, Loss: 0.198431, Accuracy: 92.70% [44800/60000]\n",
            "Batch 800, Loss: 0.200385, Accuracy: 92.60% [51200/60000]\n",
            "Batch 900, Loss: 0.199744, Accuracy: 92.64% [57600/60000]\n",
            "Epoch 19\n",
            "-------------------------------\n",
            "Batch 0, Loss: 0.239995, Accuracy: 90.62% [    0/60000]\n",
            "Batch 100, Loss: 0.206496, Accuracy: 92.13% [ 6400/60000]\n",
            "Batch 200, Loss: 0.197981, Accuracy: 92.65% [12800/60000]\n",
            "Batch 300, Loss: 0.194809, Accuracy: 92.77% [19200/60000]\n",
            "Batch 400, Loss: 0.197775, Accuracy: 92.64% [25600/60000]\n",
            "Batch 500, Loss: 0.195633, Accuracy: 92.67% [32000/60000]\n",
            "Batch 600, Loss: 0.193253, Accuracy: 92.78% [38400/60000]\n",
            "Batch 700, Loss: 0.194410, Accuracy: 92.73% [44800/60000]\n",
            "Batch 800, Loss: 0.194554, Accuracy: 92.72% [51200/60000]\n",
            "Batch 900, Loss: 0.195547, Accuracy: 92.69% [57600/60000]\n",
            "Epoch 20\n",
            "-------------------------------\n",
            "Batch 0, Loss: 0.195574, Accuracy: 92.19% [    0/60000]\n",
            "Batch 100, Loss: 0.189502, Accuracy: 93.32% [ 6400/60000]\n",
            "Batch 200, Loss: 0.183803, Accuracy: 93.44% [12800/60000]\n",
            "Batch 300, Loss: 0.184883, Accuracy: 93.37% [19200/60000]\n",
            "Batch 400, Loss: 0.185022, Accuracy: 93.27% [25600/60000]\n",
            "Batch 500, Loss: 0.184799, Accuracy: 93.31% [32000/60000]\n",
            "Batch 600, Loss: 0.185828, Accuracy: 93.24% [38400/60000]\n",
            "Batch 700, Loss: 0.185975, Accuracy: 93.22% [44800/60000]\n",
            "Batch 800, Loss: 0.185567, Accuracy: 93.24% [51200/60000]\n",
            "Batch 900, Loss: 0.187665, Accuracy: 93.16% [57600/60000]\n",
            "Epoch 21\n",
            "-------------------------------\n",
            "Batch 0, Loss: 0.117980, Accuracy: 98.44% [    0/60000]\n",
            "Batch 100, Loss: 0.178262, Accuracy: 93.33% [ 6400/60000]\n",
            "Batch 200, Loss: 0.179556, Accuracy: 93.39% [12800/60000]\n",
            "Batch 300, Loss: 0.183101, Accuracy: 93.22% [19200/60000]\n",
            "Batch 400, Loss: 0.184226, Accuracy: 93.20% [25600/60000]\n",
            "Batch 500, Loss: 0.182851, Accuracy: 93.29% [32000/60000]\n",
            "Batch 600, Loss: 0.184327, Accuracy: 93.25% [38400/60000]\n",
            "Batch 700, Loss: 0.183298, Accuracy: 93.32% [44800/60000]\n",
            "Batch 800, Loss: 0.184006, Accuracy: 93.28% [51200/60000]\n",
            "Batch 900, Loss: 0.185234, Accuracy: 93.20% [57600/60000]\n",
            "Epoch 22\n",
            "-------------------------------\n",
            "Batch 0, Loss: 0.163925, Accuracy: 96.88% [    0/60000]\n",
            "Batch 100, Loss: 0.182345, Accuracy: 93.38% [ 6400/60000]\n",
            "Batch 200, Loss: 0.178930, Accuracy: 93.52% [12800/60000]\n",
            "Batch 300, Loss: 0.178429, Accuracy: 93.48% [19200/60000]\n",
            "Batch 400, Loss: 0.180229, Accuracy: 93.42% [25600/60000]\n",
            "Batch 500, Loss: 0.179450, Accuracy: 93.42% [32000/60000]\n",
            "Batch 600, Loss: 0.179042, Accuracy: 93.38% [38400/60000]\n",
            "Batch 700, Loss: 0.178746, Accuracy: 93.39% [44800/60000]\n",
            "Batch 800, Loss: 0.178804, Accuracy: 93.41% [51200/60000]\n",
            "Batch 900, Loss: 0.178550, Accuracy: 93.39% [57600/60000]\n",
            "Epoch 23\n",
            "-------------------------------\n",
            "Batch 0, Loss: 0.125559, Accuracy: 92.19% [    0/60000]\n",
            "Batch 100, Loss: 0.171924, Accuracy: 93.60% [ 6400/60000]\n",
            "Batch 200, Loss: 0.167193, Accuracy: 93.66% [12800/60000]\n",
            "Batch 300, Loss: 0.172194, Accuracy: 93.54% [19200/60000]\n",
            "Batch 400, Loss: 0.170317, Accuracy: 93.64% [25600/60000]\n",
            "Batch 500, Loss: 0.170970, Accuracy: 93.65% [32000/60000]\n",
            "Batch 600, Loss: 0.169597, Accuracy: 93.65% [38400/60000]\n",
            "Batch 700, Loss: 0.170239, Accuracy: 93.65% [44800/60000]\n",
            "Batch 800, Loss: 0.170983, Accuracy: 93.65% [51200/60000]\n",
            "Batch 900, Loss: 0.173310, Accuracy: 93.54% [57600/60000]\n",
            "Epoch 24\n",
            "-------------------------------\n",
            "Batch 0, Loss: 0.099638, Accuracy: 96.88% [    0/60000]\n",
            "Batch 100, Loss: 0.190359, Accuracy: 92.93% [ 6400/60000]\n",
            "Batch 200, Loss: 0.179133, Accuracy: 93.38% [12800/60000]\n",
            "Batch 300, Loss: 0.173751, Accuracy: 93.43% [19200/60000]\n",
            "Batch 400, Loss: 0.174586, Accuracy: 93.45% [25600/60000]\n",
            "Batch 500, Loss: 0.171002, Accuracy: 93.60% [32000/60000]\n",
            "Batch 600, Loss: 0.169924, Accuracy: 93.63% [38400/60000]\n",
            "Batch 700, Loss: 0.170436, Accuracy: 93.63% [44800/60000]\n",
            "Batch 800, Loss: 0.170480, Accuracy: 93.65% [51200/60000]\n",
            "Batch 900, Loss: 0.171152, Accuracy: 93.65% [57600/60000]\n",
            "Epoch 25\n",
            "-------------------------------\n",
            "Batch 0, Loss: 0.095920, Accuracy: 98.44% [    0/60000]\n",
            "Batch 100, Loss: 0.160619, Accuracy: 94.29% [ 6400/60000]\n",
            "Batch 200, Loss: 0.171106, Accuracy: 93.87% [12800/60000]\n",
            "Batch 300, Loss: 0.171130, Accuracy: 93.81% [19200/60000]\n",
            "Batch 400, Loss: 0.169445, Accuracy: 93.79% [25600/60000]\n",
            "Batch 500, Loss: 0.167139, Accuracy: 93.88% [32000/60000]\n",
            "Batch 600, Loss: 0.166185, Accuracy: 93.93% [38400/60000]\n",
            "Batch 700, Loss: 0.166137, Accuracy: 93.87% [44800/60000]\n",
            "Batch 800, Loss: 0.165082, Accuracy: 93.90% [51200/60000]\n",
            "Batch 900, Loss: 0.166711, Accuracy: 93.81% [57600/60000]\n",
            "Epoch 26\n",
            "-------------------------------\n",
            "Batch 0, Loss: 0.181917, Accuracy: 92.19% [    0/60000]\n",
            "Batch 100, Loss: 0.155054, Accuracy: 94.42% [ 6400/60000]\n",
            "Batch 200, Loss: 0.160472, Accuracy: 94.12% [12800/60000]\n",
            "Batch 300, Loss: 0.161354, Accuracy: 94.09% [19200/60000]\n",
            "Batch 400, Loss: 0.161969, Accuracy: 94.03% [25600/60000]\n",
            "Batch 500, Loss: 0.163253, Accuracy: 94.01% [32000/60000]\n",
            "Batch 600, Loss: 0.163779, Accuracy: 94.02% [38400/60000]\n",
            "Batch 700, Loss: 0.162308, Accuracy: 94.04% [44800/60000]\n",
            "Batch 800, Loss: 0.162696, Accuracy: 93.99% [51200/60000]\n",
            "Batch 900, Loss: 0.163152, Accuracy: 93.95% [57600/60000]\n",
            "Epoch 27\n",
            "-------------------------------\n",
            "Batch 0, Loss: 0.171302, Accuracy: 92.19% [    0/60000]\n",
            "Batch 100, Loss: 0.149689, Accuracy: 94.37% [ 6400/60000]\n",
            "Batch 200, Loss: 0.156298, Accuracy: 94.27% [12800/60000]\n",
            "Batch 300, Loss: 0.156082, Accuracy: 94.25% [19200/60000]\n",
            "Batch 400, Loss: 0.157492, Accuracy: 94.16% [25600/60000]\n",
            "Batch 500, Loss: 0.156273, Accuracy: 94.15% [32000/60000]\n",
            "Batch 600, Loss: 0.157714, Accuracy: 94.11% [38400/60000]\n",
            "Batch 700, Loss: 0.158402, Accuracy: 94.11% [44800/60000]\n",
            "Batch 800, Loss: 0.157441, Accuracy: 94.18% [51200/60000]\n",
            "Batch 900, Loss: 0.158476, Accuracy: 94.13% [57600/60000]\n",
            "Epoch 28\n",
            "-------------------------------\n",
            "Batch 0, Loss: 0.101570, Accuracy: 95.31% [    0/60000]\n",
            "Batch 100, Loss: 0.152935, Accuracy: 94.25% [ 6400/60000]\n",
            "Batch 200, Loss: 0.152206, Accuracy: 94.36% [12800/60000]\n",
            "Batch 300, Loss: 0.149089, Accuracy: 94.48% [19200/60000]\n",
            "Batch 400, Loss: 0.149463, Accuracy: 94.40% [25600/60000]\n",
            "Batch 500, Loss: 0.150094, Accuracy: 94.39% [32000/60000]\n",
            "Batch 600, Loss: 0.150054, Accuracy: 94.44% [38400/60000]\n",
            "Batch 700, Loss: 0.151422, Accuracy: 94.37% [44800/60000]\n",
            "Batch 800, Loss: 0.152237, Accuracy: 94.35% [51200/60000]\n",
            "Batch 900, Loss: 0.152775, Accuracy: 94.29% [57600/60000]\n",
            "Epoch 29\n",
            "-------------------------------\n",
            "Batch 0, Loss: 0.310620, Accuracy: 90.62% [    0/60000]\n",
            "Batch 100, Loss: 0.169441, Accuracy: 93.89% [ 6400/60000]\n",
            "Batch 200, Loss: 0.160279, Accuracy: 94.26% [12800/60000]\n",
            "Batch 300, Loss: 0.156599, Accuracy: 94.41% [19200/60000]\n",
            "Batch 400, Loss: 0.154254, Accuracy: 94.47% [25600/60000]\n",
            "Batch 500, Loss: 0.153385, Accuracy: 94.47% [32000/60000]\n",
            "Batch 600, Loss: 0.151621, Accuracy: 94.46% [38400/60000]\n",
            "Batch 700, Loss: 0.150892, Accuracy: 94.45% [44800/60000]\n",
            "Batch 800, Loss: 0.150409, Accuracy: 94.44% [51200/60000]\n",
            "Batch 900, Loss: 0.152326, Accuracy: 94.35% [57600/60000]\n",
            "Epoch 30\n",
            "-------------------------------\n",
            "Batch 0, Loss: 0.254855, Accuracy: 92.19% [    0/60000]\n",
            "Batch 100, Loss: 0.144988, Accuracy: 95.02% [ 6400/60000]\n",
            "Batch 200, Loss: 0.142213, Accuracy: 94.97% [12800/60000]\n",
            "Batch 300, Loss: 0.146226, Accuracy: 94.79% [19200/60000]\n",
            "Batch 400, Loss: 0.150001, Accuracy: 94.61% [25600/60000]\n",
            "Batch 500, Loss: 0.150769, Accuracy: 94.54% [32000/60000]\n",
            "Batch 600, Loss: 0.149575, Accuracy: 94.57% [38400/60000]\n",
            "Batch 700, Loss: 0.149992, Accuracy: 94.52% [44800/60000]\n",
            "Batch 800, Loss: 0.150033, Accuracy: 94.50% [51200/60000]\n",
            "Batch 900, Loss: 0.149279, Accuracy: 94.51% [57600/60000]\n",
            "Epoch 31\n",
            "-------------------------------\n",
            "Batch 0, Loss: 0.098874, Accuracy: 96.88% [    0/60000]\n",
            "Batch 100, Loss: 0.143901, Accuracy: 94.48% [ 6400/60000]\n",
            "Batch 200, Loss: 0.144837, Accuracy: 94.61% [12800/60000]\n",
            "Batch 300, Loss: 0.143908, Accuracy: 94.61% [19200/60000]\n",
            "Batch 400, Loss: 0.142402, Accuracy: 94.68% [25600/60000]\n",
            "Batch 500, Loss: 0.145071, Accuracy: 94.63% [32000/60000]\n",
            "Batch 600, Loss: 0.145628, Accuracy: 94.56% [38400/60000]\n",
            "Batch 700, Loss: 0.145752, Accuracy: 94.60% [44800/60000]\n",
            "Batch 800, Loss: 0.145795, Accuracy: 94.56% [51200/60000]\n",
            "Batch 900, Loss: 0.145970, Accuracy: 94.58% [57600/60000]\n",
            "Epoch 32\n",
            "-------------------------------\n",
            "Batch 0, Loss: 0.237312, Accuracy: 93.75% [    0/60000]\n",
            "Batch 100, Loss: 0.137162, Accuracy: 94.79% [ 6400/60000]\n",
            "Batch 200, Loss: 0.142050, Accuracy: 94.76% [12800/60000]\n",
            "Batch 300, Loss: 0.143204, Accuracy: 94.67% [19200/60000]\n",
            "Batch 400, Loss: 0.140207, Accuracy: 94.79% [25600/60000]\n",
            "Batch 500, Loss: 0.141697, Accuracy: 94.71% [32000/60000]\n",
            "Batch 600, Loss: 0.141025, Accuracy: 94.74% [38400/60000]\n",
            "Batch 700, Loss: 0.140883, Accuracy: 94.78% [44800/60000]\n",
            "Batch 800, Loss: 0.141181, Accuracy: 94.74% [51200/60000]\n",
            "Batch 900, Loss: 0.142043, Accuracy: 94.70% [57600/60000]\n",
            "Epoch 33\n",
            "-------------------------------\n",
            "Batch 0, Loss: 0.069654, Accuracy: 98.44% [    0/60000]\n",
            "Batch 100, Loss: 0.139856, Accuracy: 94.82% [ 6400/60000]\n",
            "Batch 200, Loss: 0.131924, Accuracy: 95.10% [12800/60000]\n",
            "Batch 300, Loss: 0.137854, Accuracy: 94.82% [19200/60000]\n",
            "Batch 400, Loss: 0.134812, Accuracy: 95.00% [25600/60000]\n",
            "Batch 500, Loss: 0.136394, Accuracy: 94.94% [32000/60000]\n",
            "Batch 600, Loss: 0.138124, Accuracy: 94.87% [38400/60000]\n",
            "Batch 700, Loss: 0.139877, Accuracy: 94.77% [44800/60000]\n",
            "Batch 800, Loss: 0.139310, Accuracy: 94.79% [51200/60000]\n",
            "Batch 900, Loss: 0.139715, Accuracy: 94.71% [57600/60000]\n",
            "Epoch 34\n",
            "-------------------------------\n",
            "Batch 0, Loss: 0.143769, Accuracy: 95.31% [    0/60000]\n",
            "Batch 100, Loss: 0.142196, Accuracy: 94.85% [ 6400/60000]\n",
            "Batch 200, Loss: 0.136182, Accuracy: 95.19% [12800/60000]\n",
            "Batch 300, Loss: 0.133372, Accuracy: 95.23% [19200/60000]\n",
            "Batch 400, Loss: 0.134844, Accuracy: 95.17% [25600/60000]\n",
            "Batch 500, Loss: 0.136666, Accuracy: 95.09% [32000/60000]\n",
            "Batch 600, Loss: 0.137610, Accuracy: 95.01% [38400/60000]\n",
            "Batch 700, Loss: 0.137188, Accuracy: 95.02% [44800/60000]\n",
            "Batch 800, Loss: 0.136024, Accuracy: 95.03% [51200/60000]\n",
            "Batch 900, Loss: 0.137702, Accuracy: 94.95% [57600/60000]\n",
            "Epoch 35\n",
            "-------------------------------\n",
            "Batch 0, Loss: 0.089128, Accuracy: 96.88% [    0/60000]\n",
            "Batch 100, Loss: 0.142517, Accuracy: 94.48% [ 6400/60000]\n",
            "Batch 200, Loss: 0.133638, Accuracy: 95.04% [12800/60000]\n",
            "Batch 300, Loss: 0.132354, Accuracy: 95.09% [19200/60000]\n",
            "Batch 400, Loss: 0.131139, Accuracy: 95.18% [25600/60000]\n",
            "Batch 500, Loss: 0.133894, Accuracy: 95.08% [32000/60000]\n",
            "Batch 600, Loss: 0.132805, Accuracy: 95.13% [38400/60000]\n",
            "Batch 700, Loss: 0.132094, Accuracy: 95.17% [44800/60000]\n",
            "Batch 800, Loss: 0.131886, Accuracy: 95.15% [51200/60000]\n",
            "Batch 900, Loss: 0.133355, Accuracy: 95.09% [57600/60000]\n",
            "Reached 95% accuracy, stopping training.\n",
            "Early stopping triggered.\n",
            "Done!\n"
          ]
        }
      ],
      "source": [
        "# EARLY STOPPING VERSION\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torchvision import datasets, transforms\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "# Load the dataset\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor()  # Automatically converts to tensor and scales to [0, 1]\n",
        "])\n",
        "train_dataset = datasets.FashionMNIST(root='./data', train=True, download=True, transform=transform)\n",
        "test_dataset = datasets.FashionMNIST(root='./data', train=False, download=True, transform=transform)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)\n",
        "\n",
        "# Define the model\n",
        "class FashionMNISTModel(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(FashionMNISTModel, self).__init__()\n",
        "        self.flatten = nn.Flatten()\n",
        "        self.linear_relu_stack = nn.Sequential(\n",
        "            nn.Linear(28*28, 128),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(128, 10),\n",
        "            nn.LogSoftmax(dim=1)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.flatten(x)\n",
        "        logits = self.linear_relu_stack(x)\n",
        "        return logits\n",
        "\n",
        "model = FashionMNISTModel()\n",
        "if torch.cuda.is_available():\n",
        "    model = model.to('cuda')\n",
        "\n",
        "# Define the loss function and optimizer\n",
        "loss_function = nn.NLLLoss()\n",
        "optimizer = optim.Adam(model.parameters())\n",
        "\n",
        "# Function to calculate accuracy\n",
        "def get_accuracy(pred, labels):\n",
        "    _, predictions = torch.max(pred, 1)\n",
        "    correct = (predictions == labels).float().sum()\n",
        "    accuracy = correct / labels.shape[0]\n",
        "    return accuracy\n",
        "\n",
        "# Train the model with accuracy reporting\n",
        "def train(dataloader, model, loss_fn, optimizer):\n",
        "    size = len(dataloader.dataset)\n",
        "    num_batches = len(dataloader)\n",
        "    total_loss, total_accuracy = 0, 0\n",
        "\n",
        "    model.train()\n",
        "    for batch, (X, y) in enumerate(dataloader):\n",
        "        if torch.cuda.is_available():\n",
        "            X, y = X.to('cuda'), y.to('cuda')\n",
        "        # Compute prediction and loss\n",
        "        pred = model(X)\n",
        "        loss = loss_fn(pred, y)\n",
        "        accuracy = get_accuracy(pred, y)\n",
        "\n",
        "        # Backpropagation\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "        total_accuracy += accuracy.item()\n",
        "\n",
        "        if batch % 100 == 0:\n",
        "            current = batch * len(X)\n",
        "            avg_loss = total_loss / (batch + 1)\n",
        "            avg_accuracy = total_accuracy / (batch + 1) * 100\n",
        "            print(f\"Batch {batch}, Loss: {avg_loss:>7f}, Accuracy: {avg_accuracy:>0.2f}% [{current:>5d}/{size:>5d}]\")\n",
        "\n",
        "    # Early stopping condition\n",
        "    if avg_accuracy >= 95:\n",
        "        print(\"Reached 95% accuracy, stopping training.\")\n",
        "        return True  # Stop training\n",
        "\n",
        "# Training process\n",
        "epochs = 50\n",
        "for t in range(epochs):\n",
        "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
        "    if train(train_loader, model, loss_function, optimizer):  # Check for the early stopping signal\n",
        "        print(\"Early stopping triggered.\")\n",
        "        break\n",
        "print(\"Done!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "torch",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
